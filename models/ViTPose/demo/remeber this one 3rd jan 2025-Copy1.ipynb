{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630346d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obafemi/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1+cu110 True\n",
      "torchvision version: 0.8.2+cu110\n",
      "mmpose version: 0.24.0\n",
      "cuda version: 11.7\n",
      "compiler information: GCC 9.4\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "Use load_from_local loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obafemi/anaconda3/lib/python3.9/site-packages/mmdet/core/anchor/builder.py:15: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_http loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obafemi/anaconda3/lib/python3.9/site-packages/mmdet/core/anchor/anchor_generator.py:323: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n",
      "  warnings.warn('``grid_anchors`` would be deprecated soon. '\n",
      "/home/obafemi/anaconda3/lib/python3.9/site-packages/mmdet/core/anchor/anchor_generator.py:359: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 146>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m h, w, _ \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m#if h * w < 256 * 256:\u001b[39;00m\n\u001b[1;32m    156\u001b[0m  \u001b[38;5;66;03m#   continue\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m mmdet_results \u001b[38;5;241m=\u001b[39m \u001b[43minference_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m final_det \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    161\u001b[0m person_results \u001b[38;5;241m=\u001b[39m process_mmdet_results(mmdet_results, cat_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/apis/inference.py:147\u001b[0m, in \u001b[0;36minference_detector\u001b[0;34m(model, imgs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 147\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batch:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py:97\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@auto_fp16 can only be used to decorate the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     95\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod of nn.Module\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfp16_enabled):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[1;32m    100\u001b[0m args_info \u001b[38;5;241m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/models/detectors/base.py:173\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_train(img, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/models/detectors/base.py:146\u001b[0m, in \u001b[0;36mBaseDetector.forward_test\u001b[0;34m(self, imgs, img_metas, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    145\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m imgs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maug test does not support \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    149\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference with batch size \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    150\u001b[0m                                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/models/detectors/two_stage.py:177\u001b[0m, in \u001b[0;36mTwoStageDetector.simple_test\u001b[0;34m(self, img, img_metas, proposals, rescale)\u001b[0m\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_feat(img)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proposals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     proposal_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_test_rpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     proposal_list \u001b[38;5;241m=\u001b[39m proposals\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/models/dense_heads/dense_test_mixins.py:124\u001b[0m, in \u001b[0;36mBBoxTestMixin.simple_test_rpn\u001b[0;34m(self, x, img_metas)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"Test without augmentation, only for ``RPNHead`` and its variants,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03me.g., ``GARPNHead``, etc.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m        where 5 represent (tl_x, tl_y, br_x, br_y, score).\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m rpn_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m--> 124\u001b[0m proposal_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrpn_outs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proposal_list\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py:184\u001b[0m, in \u001b[0;36mforce_fp32.<locals>.force_fp32_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@force_fp32 can only be used to decorate the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod of nn.Module\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfp16_enabled):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[1;32m    186\u001b[0m args_info \u001b[38;5;241m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/models/dense_heads/rpn_head.py:115\u001b[0m, in \u001b[0;36mRPNHead.get_bboxes\u001b[0;34m(self, cls_scores, bbox_preds, img_metas, cfg, rescale, with_nms)\u001b[0m\n\u001b[1;32m    113\u001b[0m device \u001b[38;5;241m=\u001b[39m cls_scores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    114\u001b[0m featmap_sizes \u001b[38;5;241m=\u001b[39m [cls_scores[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_levels)]\n\u001b[0;32m--> 115\u001b[0m mlvl_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_anchors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatmap_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(img_metas)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/mmdet/core/anchor/anchor_generator.py:330\u001b[0m, in \u001b[0;36mAnchorGenerator.grid_anchors\u001b[0;34m(self, featmap_sizes, device)\u001b[0m\n\u001b[1;32m    327\u001b[0m multi_level_anchors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_levels):\n\u001b[1;32m    329\u001b[0m     anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_level_grid_anchors(\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_anchors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    331\u001b[0m         featmap_sizes[i],\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides[i],\n\u001b[1;32m    333\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    334\u001b[0m     multi_level_anchors\u001b[38;5;241m.\u001b[39mappend(anchors)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multi_level_anchors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import torch, torchvision\n",
    "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmpose\n",
    "print('mmpose version:', mmpose.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print('cuda version:', get_compiling_cuda_version())\n",
    "print('compiler information:', get_compiler_version())\n",
    "\n",
    "\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results,inference_top_down_pose_model_modified\n",
    "                        ,vis_pose_result_modified)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "local_runtime = False\n",
    "\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "import tempfile\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "categories = [{'supercategory': 'animal',\n",
    "  'id': 1,\n",
    "  'name': 'animal',\n",
    "  'keypoints': ['bottom_trunk',\n",
    "   'mid_trunk',\n",
    "   'top_trunk',\n",
    "   'bottom_right_tusk',\n",
    "   'bottom_left_tusk',\n",
    "   'top_right_tusk',\n",
    "   'top_left_tusk',\n",
    "   'right_eye',\n",
    "   'left_eye',\n",
    "   'right_bottom_ear',\n",
    "   'left_bottom_ear',\n",
    "   'right_bottom_tip_ear',\n",
    "   'left_bottom_tip_ear',\n",
    "   'right_side_tip_ear',\n",
    "   'left_side_tip_ear',\n",
    "   'top_right_ear',\n",
    "   'top_left_ear',\n",
    "   'top_right_tip_ear',\n",
    "   'top_left_tip_ear',\n",
    "   'hoof',\n",
    "   'tail',\n",
    "   'right_front_elbow',\n",
    "   'left_front_elbow',\n",
    "   'right_back_elbow',\n",
    "   'left_back_elbow',\n",
    "   'right_front_knee',\n",
    "   'left_front_knee',\n",
    "   'right_back_knee',\n",
    "   'left_back_knee',\n",
    "   'right_front_foot',\n",
    "   'left_front_foot',\n",
    "   'right_back_foot',\n",
    "   'left_back_foot'],\n",
    "  'skeleton': [[1, 2],\n",
    "   [2, 3],\n",
    "   [3, 8],\n",
    "   [3, 9],\n",
    "   [8, 9],\n",
    "   [8, 16],\n",
    "   [9, 17],\n",
    "   [3, 10],\n",
    "   [3, 11],\n",
    "   [10, 20],\n",
    "   [11, 20],\n",
    "   [10, 22],\n",
    "   [11, 22],\n",
    "   [10, 23],\n",
    "   [11, 23],\n",
    "   [22, 26],\n",
    "   [23, 27],\n",
    "   [26, 30],\n",
    "   [27, 31],\n",
    "   [20, 21],\n",
    "   [21, 24],\n",
    "   [21, 25],\n",
    "   [24, 28],\n",
    "   [25, 29],\n",
    "   [28, 32],\n",
    "   [29, 33]]}]\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "# initialize pose model\n",
    "\n",
    "#pose_config = '/media/obafemi/New Volume/ViTPose/vitpose_b_elephant.py'\n",
    "#pose_config = '/media/obafemi/New Volume/ViTPose/vitpose_b_femi_animalpose.py'\n",
    "#pose_checkpoint = '/media/obafemi/New Volume/ViTPose/work_dirs/vitpose_b_femi_animalpose/best_AP_epoch_200.pth'\n",
    "\n",
    "\n",
    "pose_config = '/media/obafemi/New Volume/ViTPose/vitpose_b_elephant_teacher.py'\n",
    "pose_checkpoint = '/media/obafemi/New Volume/ViTPose/work_dirs/vitpose_b_elephant_teacher_lr_reduced earlier/best_AP_epoch_330.pth'\n",
    "det_config = 'mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# initialize detector\n",
    "\n",
    "det_model = init_detector(det_config, det_checkpoint)\n",
    "pseudo = '/media/obafemi/New Volume/ViTPose/data/elephant_pseudo/data/train2017'\n",
    "#pseudo = '/media/obafemi/New Volume/ViTPose/demo/rhino'\n",
    "\n",
    "#classes = os.listdir('/media/obafemi/New Volume/ViTPose/unlabeled_dataset')\n",
    "#classes= ['elephants']\n",
    "\n",
    "category_id = 0\n",
    "is_crowd = 0\n",
    "\n",
    "\n",
    "pseudo_image_list = []\n",
    "pseudo_annotations = []\n",
    "\n",
    "M = 0\n",
    "\n",
    "imgs_in = os.listdir(pseudo)\n",
    "\n",
    "for i, im_name in enumerate(imgs_in):\n",
    "   \n",
    "    image_name_ = imgs_in[i]\n",
    "    image_name = os.path.join(pseudo, image_name_)\n",
    "\n",
    "    img = cv2.imread(image_name)\n",
    "    if img is None:\n",
    "        print(image_name)\n",
    "    h, w, _ = img.shape\n",
    "    #if h * w < 256 * 256:\n",
    "     #   continue\n",
    "    mmdet_results = inference_detector(det_model, img)\n",
    "    \n",
    "    final_det = []\n",
    "   \n",
    "    person_results = process_mmdet_results(mmdet_results, cat_id=21)\n",
    "\n",
    "    pose_results, returned_outputs = inference_top_down_pose_model_modified(pose_model,\n",
    "                                                                   img,\n",
    "                                                                   person_results,\n",
    "                                                                   bbox_thr=0.3,\n",
    "                                                                   format='xyxy',\n",
    "                                                                    cat_id=0\n",
    "                                                                           )#dataset=pose_model.cfg.data.test.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de24a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1+cu110 True\n",
      "torchvision version: 0.8.2+cu110\n",
      "mmpose version: 0.24.0\n",
      "cuda version: 11.7\n",
      "compiler information: GCC 9.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/media/obafemi/New Volume/Evaluate_animal_pose_with_different_models')\n",
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import *\n",
    "from generateReal import *\n",
    "from generateSynthetic import *\n",
    "from generatePseudos import *\n",
    "\n",
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmpose\n",
    "print('mmpose version:', mmpose.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print('cuda version:', get_compiling_cuda_version())\n",
    "print('compiler information:', get_compiler_version())\n",
    "\n",
    "import cv2\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results,inference_top_down_pose_model_modified\n",
    "                        ,vis_pose_result_modified)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "local_runtime = False\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import time\n",
    "\n",
    "from models.hrnet import HRNet\n",
    "from models.poseresnet import PoseResNet\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords, xyxy2xywh\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "from utils.inference import *\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "import tempfile\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34217a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_bboxes(img0, weights='/media/obafemi/New Volume/Evaluate_animal_pose_with_different_models/yolo_weights/yolov5x.pt', classes=list(range(0, 80))):\n",
    "\n",
    "        imgsz = 640\n",
    "        device = '0'\n",
    "\n",
    "        # Initialize\n",
    "        device = select_device(device)  # make new output folder\n",
    "\n",
    "        # Load model\n",
    "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "\n",
    "        half = False\n",
    "        # Get names and colors\n",
    "        names = model.module.names if hasattr(model, 'module') else model.names\n",
    "        #class_map = names.index(classes)\n",
    "        a = dict(zip(names, np.arange(80)))\n",
    "        class_map = list(a.keys())\n",
    "        #print(class_map)\n",
    "        #class_map = classes\n",
    "        # Run inference\n",
    "        t0 = time.time()\n",
    "        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "        _ = model(img.half() if half else img) if device.type != 'cpu' else None\n",
    "        bboxes = []  # run once\n",
    "\n",
    "        img = letterbox(img0, new_shape=imgsz)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()\n",
    "        img /= 255.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "            # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=True)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, 0.6, 0.5, classes=classes, agnostic=True)  # opt.classes\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            s, im0 = '', img0\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                #print(det[:,-1].item()) #############################\n",
    "                # Print results\n",
    "                #print(det)\n",
    "                for c in det[:, -1]:\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    #print(n)\n",
    "                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "\n",
    "                # Append results\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    #print(,conf,cls)\n",
    "                    #f cls == class_map:\n",
    "                     #   print('here')\n",
    "                    dic = {'bbox':np.array([xyxy[0].item(), xyxy[1].item(), xyxy[2].item(),xyxy[3].item(),conf.item()],dtype = 'float32'),'cls':np.array(cls.item())}\n",
    "                    #bboxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3]),conf.item()])\n",
    "                    bboxes.append(dic)\n",
    "                    #bboxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3]),conf.item()])\n",
    "\n",
    "        return bboxes\n",
    "    \n",
    "def get_catID(cls):\n",
    "    cat_id = 0 if cls == 20 else 1\n",
    "    \n",
    "    return cat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e32d2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1+cu110 True\n",
      "torchvision version: 0.8.2+cu110\n",
      "mmpose version: 0.24.0\n",
      "cuda version: 11.7\n",
      "compiler information: GCC 9.4\n",
      "Use load_from_local loader\n",
      "Use load_from_http loader\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import cv2\n",
    "import shutil\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import torch, torchvision\n",
    "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "\n",
    "# Check MMPose installation\n",
    "import mmpose\n",
    "print('mmpose version:', mmpose.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print('cuda version:', get_compiling_cuda_version())\n",
    "print('compiler information:', get_compiler_version())\n",
    "\n",
    "\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results,inference_top_down_pose_model_modified\n",
    "                        ,vis_pose_result_modified)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "local_runtime = False\n",
    "\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "import tempfile\n",
    "import os.path as osp\n",
    "\n",
    "# initialize pose model\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "def process_mmdet_results_modified(mmdet_results):\n",
    "    \"\"\"Process mmdet results, and return a list of bboxes.\n",
    "\n",
    "    Args:\n",
    "        mmdet_results (list|tuple): mmdet results.\n",
    "        cat_id (int): category id (default: 1 for human)\n",
    "\n",
    "    Returns:\n",
    "        person_results (list): a list of detected bounding boxes\n",
    "    \"\"\"\n",
    "    if isinstance(mmdet_results, tuple):\n",
    "        det_results = mmdet_results[0]\n",
    "    else:\n",
    "        det_results = mmdet_results\n",
    "    \n",
    "    det_results.pop(20)\n",
    "    det_results.pop(0)\n",
    "    \n",
    "    bboxes = np.zeros((1,5))\n",
    "    \n",
    "    for i in range(len(det_results)):\n",
    "        bboxes = np.vstack((bboxes,det_results[i]))\n",
    "        \n",
    "    #bboxes = det_results[0]\n",
    "    bboxes = bboxes[1:,:]\n",
    "    person_results = []\n",
    "    for bbox in bboxes:\n",
    "        person = {}\n",
    "        person['bbox'] = bbox\n",
    "        person_results.append(person)\n",
    "\n",
    "    return person_results\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "pose_config = '/media/obafemi/New Volume/ViTPose/ViTPose_large_ap10k_256x192.py'\n",
    "pose_checkpoint = '/media/obafemi/New Volume/ViTPose/work_dirs/epoch_500.pth'\n",
    "det_config = 'mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
    "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# initialize detector\n",
    "\n",
    "det_model = init_detector(det_config, det_checkpoint)\n",
    "pseudo = '/media/obafemi/New Volume/ViTPose/Deer Dataset Sample/new data/image'\n",
    "\n",
    "\n",
    "\n",
    "imgs_in = os.listdir(pseudo)\n",
    "#print(imgs_in)\n",
    "for i, im_name in enumerate(imgs_in):\n",
    "    #print(i, im_name)\n",
    "\n",
    "    image_name_ = imgs_in[i]\n",
    "    image_name = os.path.join(pseudo, image_name_)\n",
    " \n",
    "    img = cv2.imread(image_name)\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    #if h * w < 256 * 256:\n",
    "    # continue\n",
    "    mmdet_results = inference_detector(det_model, img)\n",
    "    person_results = process_mmdet_results_modified(mmdet_results)\n",
    "    \n",
    "    \n",
    "    #person_results = process_mmdet_results(mmdet_results, cat_id=0)\n",
    "    final_det = get_bboxes(img)\n",
    "    person_results = final_det\n",
    "\n",
    "    if len(person_results)==0:\n",
    "        continue\n",
    "    pose_results, returned_outputs = inference_top_down_pose_model_modified(pose_model,\n",
    "                                                                img,\n",
    "                                                                person_results,\n",
    "                                                                bbox_thr=0.3,\n",
    "                                                                format='xyxy',\n",
    "                                                                    cat_id=1\n",
    "                                                                        )\n",
    "\n",
    "     # show pose estimation results\n",
    "    vis_result = vis_pose_result_modified(pose_model,\n",
    "                                 img,\n",
    "                                pose_results,\n",
    "                                          cat_id=1,\n",
    "                                          #cat_id=1,\n",
    "                                          kpt_score_thr=0.42,\n",
    "                                #dataset=pose_model.cfg.data.test.type,\n",
    "                                 show=False)\n",
    "    # reduce image size\n",
    "     # reduce image size\n",
    "    vis_result = cv2.resize(vis_result, dsize=None, fx=0.5, fy=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    #with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    file_name = osp.join('/media/obafemi/New Volume/ViTPose/demo/deer_poses', 'pose_results'+str(i)+'.png')\n",
    "    #print(file_name)\n",
    "    cv2.imwrite(file_name, vis_result)\n",
    "    #display(Image(file_name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021a6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9525eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff408d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_top_down_pose_model_modified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53753142",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pose_result_modified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62937c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
