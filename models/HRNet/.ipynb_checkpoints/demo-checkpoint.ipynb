{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## An example of model inference and visualization\n",
    "#### Prepare the input data and model\n",
    "\n",
    "We generate the cropped images from COCO2017 valiadation set by using the GT human bounding boxes. And we load the model with the COCO pre-trained weights. The models also can be directly loaded from torch hub ([CoLab demo](https://colab.research.google.com/drive/1v2LY_rAZXqexPjiePmqgma4aw-qmRek6?usp=sharing))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path as osp\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from visualize import update_config, add_path\n",
    "\n",
    "lib_path = osp.join('lib')\n",
    "add_path(lib_path)\n",
    "\n",
    "import dataset as dataset\n",
    "from config import cfg\n",
    "import models\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "file_name = 'experiments/coco/transpose_h/TP_H_w48_256x192_stage3_1_4_d192_h384_relu_enc4_mh1.yaml' # choose a yaml file\n",
    "f = open(file_name, 'r')\n",
    "update_config(cfg, file_name)\n",
    "\n",
    "model_name = 'T-H-A4'\n",
    "assert model_name in ['T-R', 'T-H','T-H-L','T-R-A4', 'T-H-A6', 'T-H-A5', 'T-H-A4' ,'T-R-A4-DirectAttention']\n",
    "\n",
    "normalize = T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "        cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False, 'real',\n",
    "        T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    )\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=True\n",
    ")\n",
    "\n",
    "if cfg.TEST.MODEL_FILE:\n",
    "    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=True)\n",
    "else:\n",
    "    raise ValueError(\"please choose one ckpt in cfg.TEST.MODEL_FILE\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"model params:{:.3f}M\".format(sum([p.numel() for p in model.parameters()])/1000**2))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Obtain the predictions\n",
    "\n",
    "We feed an input sample to the model, get the predicted keypoint heatmaps and further obtain the positions of keypoints. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \n",
    "from lib.core.inference import get_final_preds\n",
    "from lib.utils import transforms, vis\n",
    "import cv2\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    tmp = []\n",
    "    tmp2 = []\n",
    "    img = dataset[0][0]\n",
    "    print(\"img\", img.shape)\n",
    "\n",
    "    inputs = torch.cat([img.to(device)]).unsqueeze(0)\n",
    "    print(\"inp\", inputs.shape)\n",
    "    outputs = model(inputs)\n",
    "    if isinstance(outputs, list):\n",
    "        output = outputs[-1]\n",
    "    else:\n",
    "        output = outputs\n",
    "\n",
    "    if cfg.TEST.FLIP_TEST: \n",
    "        input_flipped = np.flip(inputs.cpu().numpy(), 3).copy()\n",
    "        input_flipped = torch.from_numpy(input_flipped).cuda()\n",
    "        outputs_flipped = model(input_flipped)\n",
    "\n",
    "        if isinstance(outputs_flipped, list):\n",
    "            output_flipped = outputs_flipped[-1]\n",
    "        else:\n",
    "            output_flipped = outputs_flipped\n",
    "\n",
    "        output_flipped = transforms.flip_back(output_flipped.cpu().numpy(),\n",
    "                                   dataset.flip_pairs)\n",
    "        output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\n",
    "\n",
    "        output = (output + output_flipped) * 0.5\n",
    "        \n",
    "    preds, maxvals = get_final_preds(\n",
    "            cfg, output.clone().cpu().numpy(), None, None, transform_back=False)\n",
    "\n",
    "# from heatmap_coord to original_image_coord\n",
    "query_locations = np.array([p*4+0.5 for p in preds[0]])\n",
    "print(query_locations)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Obtain the attention maps and visualize them\n",
    "The attention maps can be returned by the forward function of the model, or can be hooked by the hook function of pytorch. Here we conduct the hook implementation in the function `inspect_atten_map_by_locations`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from visualize import inspect_atten_map_by_locations\n",
    "\n",
    "inspect_atten_map_by_locations(img, model, query_locations, model_name=\"transposer\", mode='dependency', save_img=True, threshold=0.0)\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "99ac092c052c335a585d60519d25aff84681decc510ed8ced49adeb4d0f2067a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}